input {
  file {
    path => "/var/log/**/*.log"
    exclude => "*.gz"
    start_position => "beginning"
    sincedb_path => "/usr/share/logstash/data/sincedb"
    sincedb_write_interval => 15
    type => "file"
    add_field => [ "pipeline", "file" ]
  }
}

filter {
  # Parse JSON structured logs (common in microservices)
  if [message] =~ /^\{/ {
    json {
      source => "message"
      target => "json_data"
      skip_on_invalid_json => true
    }

    # Extract common fields from JSON logs
    if [json_data][level] {
      mutate {
        add_field => [ "log_level", "%{[json_data][level]}" ]
      }
    }

    if [json_data][service] {
      mutate {
        add_field => [ "service_name", "%{[json_data][service]}" ]
      }
    }

    if [json_data][timestamp] {
      date {
        match => [ "[json_data][timestamp]", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'" ]
        target => "@timestamp"
      }
    }
  }

  # Add file metadata
  mutate {
    add_field => [ "log_file", "%{path}" ]
    add_field => [ "received_at", "%{@timestamp}" ]
  }

  # Clean up file path for readability
  mutate {
    gsub => [ "log_file", "/var/log/", "" ]
  }
}

output {
  elasticsearch {
    hosts => ["https://es01:9200"]
    index => "logstash-file-%{+YYYY.MM.dd}"
    user => "elastic"
    password => "${ELASTIC_PASSWORD}"
    ssl_enabled => true
    ssl_certificate_authorities => ["/usr/share/logstash/config/certs/ca/ca.crt"]
    ssl_verification_mode => "full"
  }
  stdout {
    codec => rubydebug
    id => "file-debug"
  }
}